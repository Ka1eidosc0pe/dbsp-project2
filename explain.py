import psycopg2
import json
import math
import re

# Global variables
node_number = 1
explanation_node_number = 1


class QueryPlanNode:
    def __init__(self, node_data, parent=None):
        # Node type identifies the algorithm used in this node
        self.node_type = node_data['Node Type']

        # Parent and child describes the parent node of this node, and the children nodes of this node
        self.parent = parent
        self.children = [QueryPlanNode(child_data, parent=self) for child_data in node_data.get('Plans', [])]

        # Info related to leaf nodes of query plan tree (scans)
        self.is_leaf = len(self.children) == 0
        self.relation_name = node_data.get('Relation Name')
        self.filter = node_data.get('Filter')
        self.index_name = node_data.get('Index Name')
        self.index_cond = node_data.get('Index Cond')
        self.hash_cond = node_data.get('Hash Cond')

        # Info related to estimated costs generated by psql
        self.startup_cost = node_data.get('Startup Cost')
        self.total_cost = node_data.get('Total Cost')
        self.plan_rows = node_data.get('Plan Rows')
        self.plan_width = node_data.get('Plan Width')

        # Our estimated cost and explanation for the node
        self.calculated_cost = 0.0
        self.calculation_explain = ""


def build_query_plan_tree(explain_data):
    return QueryPlanNode(explain_data['Plan'])


def print_query_plan_tree(node, indent=0):
    if node.relation_name and node.filter:
        relation_info = f" (Relation: {node.relation_name}, Filter: {node.filter})"
    elif node.relation_name:
        relation_info = f" (Relation: {node.relation_name}, No Filter)"
    else:
        relation_info = ""
    print('  ' * indent + f"{node.node_type}{relation_info}, Total Cost: {node.total_cost} ")
    for child in node.children:
        print_query_plan_tree(child, indent + 1)


def get_query_plan_tree(node, indent=0):
    output = ""

    if node.relation_name and node.filter:
        relation_info = f" (Relation: {node.relation_name}, Filter: {node.filter})"
    elif node.relation_name:
        relation_info = f" (Relation: {node.relation_name}, No Filter)"
    else:
        relation_info = ""

    output += '  ' * indent + f"{node.node_type}{relation_info}, Total Cost: {node.total_cost}\n"

    for child in node.children:
        output += get_query_plan_tree(child, indent + 1)

    return output


def get_costs_info(node):
    global node_number
    output = f"""
    Node {node_number}: {node.node_type}
    Actual Cost: {node.total_cost} vs Calculated Cost: {node.calculated_cost}\n"""
    node_number += 1

    for child in node.children:
        output += get_costs_info(child)

    return output


def get_cost_explanation(node):
    global explanation_node_number
    output = f"""
        Node {explanation_node_number}: {node.node_type}
        Explanation:{node.calculation_explain}\n"""
    explanation_node_number += 1

    for child in node.children:
        output += get_cost_explanation(child)

    return output


# Calculate the costs of every node of the query plan tree
def calculate_cost(connection, node):
    # If the node is a leaf node, it must be a scan node
    # Go ahead and calculate the cost of this node and generate an explanation for it
    if node.is_leaf:
        match node.node_type:
            case 'Seq Scan':
                compute_seq_scan_cost(connection, node)
            case 'Index Scan':
                compute_index_scan_cost(connection, node)
            case 'Index Only Scan':
                compute_index_only_scan_cost(connection, node)
            case 'Bitmap Index Scan':
                compute_bitmap_index_scan_cost(connection, node)

    # If the node is not a leaf node, then recursively call calculate_cost on each child node until we reach a leaf node
    # All nodes on the outer loop will be processed first
    else:
        for child in node.children:
            calculate_cost(connection, child)

        # Once it has returned from the recursion, it means that all of its children nodes' costs have been calculated
        # Depending on the node type, calculate the cost for this node and generate an explanation for it
        match node.node_type:
            case 'Bitmap Heap Scan':
                compute_bitmap_heap_scan_cost(connection, node)
            case 'Nested Loop' | 'Nested Loop Left Join' | 'Nested Loop Right Join' | 'Nested Loop Inner Join':
                compute_nested_loop_cost(connection, node)
            case 'Hash':
                compute_hash_cost(node)
            case 'Hash Join' | 'Hash Left Join' | 'Hash Right Join' | 'Hash Inner Join':
                compute_hash_join_cost(connection, node)
            case 'Hash Semi Join':
                compute_hash_semi_join_cost(connection, node)
            case 'Merge Join' | 'Merge Left Join' | 'Merge Right Join' | 'Merge Inner Join':
                compute_merge_join_cost(connection, node)
            case 'Sort':
                compute_sort_cost(connection, node)
            case 'Aggregate':
                compute_aggregate_cost(connection, node)
            case 'Materialize':
                compute_materialize_cost(connection, node)
            case 'Memoize':
                compute_memoize_cost(connection, node)


# Functions to estimate cost of scan nodes
def compute_seq_scan_cost(connection, node):
    # 1. Get necessary statistics and parameter values from the database
    cursor = connection.cursor()
    no_tuples, no_blocks = get_no_of_tuples_and_blocks(connection, node.relation_name)

    param_query = f"""
                SELECT name, setting
                FROM pg_settings
                WHERE name IN ('seq_page_cost', 'cpu_tuple_cost','cpu_operator_cost');
            """
    cursor.execute(param_query)
    param_result = cursor.fetchall()
    cpu_operator_cost = float(param_result[0][1])
    cpu_tuple_cost = float(param_result[1][1])
    seq_page_cost = float(param_result[2][1])

    # 2. Using the statistics and parameters retrieved, calculate the cost of the sequential scan
    # For scans with filters and no filters, the calculations for each are slightly different
    if node.filter:
        # Find number of conditions in Filter field of the Seq Scan node
        filter_expression = node.filter
        and_count = filter_expression.count('AND')
        or_count = filter_expression.count('OR')
        filter_condition_count = and_count + or_count + 1

        # Then calculate the cost
        node.calculated_cost = (no_blocks * seq_page_cost) + (no_tuples * cpu_tuple_cost) + (
                    (no_tuples * cpu_operator_cost) * filter_condition_count)
        if abs(node.total_cost - node.calculated_cost) <= 100:
            node.calculation_explain = f"""
        The database performs sequential scan on the {node.relation_name} table.

        It scans all {no_blocks} blocks of the {node.relation_name} table, then the CPU processes all {no_tuples} tuples in the table. 
        To reflect the cost of the filter, extra CPU processing cost of {cpu_operator_cost} is added for each tuple scanned. 

        Cost: (no_of_blocks * seq_page_cost) + (no_of_tuples_scanned * cpu_tuple_cost) + (no_of_tuples_scanned * cpu_operator_cost)
            """
        else:
            node.calculation_explain = f"""
        The database performs sequential scan on the {node.relation_name} table.

        It scans all {no_blocks} blocks of the {node.relation_name} table, then the CPU processes all {no_tuples} tuples in the table. 
        To reflect the cost of the filter, extra CPU processing cost of {cpu_operator_cost} is added for each tuple scanned. 

        Cost: (no_of_blocks * seq_page_cost) + (no_of_tuples_scanned * cpu_tuple_cost) + (no_of_tuples_scanned * cpu_operator_cost)
                        """
    else:
        node.calculated_cost = (no_blocks * seq_page_cost) + (no_tuples * cpu_tuple_cost)
        if abs(node.total_cost - node.calculated_cost) <= 100:
            node.calculation_explain = f"""
        The database performs sequential scan on the {node.relation_name} table.

        It scans all {no_blocks} blocks of the {node.relation_name} table, then the CPU processes all {no_tuples} tuples in the table.

        Cost: (no_of_blocks * seq_page_cost) + (no_of_tuples_scanned * cpu_tuple_cost)        
            """
        else:
            node.calculation_explain = f"""
        The database performs sequential scan on the {node.relation_name} table.

        It scans all {no_blocks} blocks of the {node.relation_name} table, then the CPU processes all {no_tuples} tuples in the table.

        Cost: (no_of_blocks * seq_page_cost) + (no_of_tuples_scanned * cpu_tuple_cost)  
                                """

    print("Actual cost for node " + node.node_type + ": " + str(node.total_cost))
    print("Calculated cost for node " + node.node_type + ": " + str(node.calculated_cost))


def compute_index_scan_cost(connection, node):
    # 1. Get necessary statistics and parameter values from the database
    cursor = connection.cursor()

    # Get number of tuples and pages of the table
    table_tuples, table_blocks = get_no_of_tuples_and_blocks(connection, node.relation_name)

    # Get number of tuples and pages of the index
    index_tuples, index_blocks = get_no_of_tuples_and_blocks(connection, node.index_name)

    # Get random_page_cost, cpu_index_tuple_cost, cpu_operator_cost, seq_page_cost and cpu_tuple_cost
    param_query = f"""
                    SELECT name, setting
                    FROM pg_settings
                    WHERE name IN ('seq_page_cost', 'cpu_tuple_cost','cpu_operator_cost','cpu_index_tuple_cost','random_page_cost');
                """
    cursor.execute(param_query)
    param_result = cursor.fetchall()
    cpu_index_tuple_cost = float(param_result[0][1])
    cpu_operator_cost = float(param_result[1][1])
    cpu_tuple_cost = float(param_result[2][1])
    random_page_cost = float(param_result[3][1])
    seq_page_cost = float(param_result[4][1])

    # Find both selectivity and correlation - if no index condition and if there is index condition
    if not (node.index_cond):
        # If no index conditions, means that it is selecting everything (similar to sequential scan)
        selectivity = 1
        correlation = 1
        predicate_symbol = "="
    else:
        condition_array = parse_condition_string(node.index_cond)
        selectivity = determine_selectivity(connection, node.relation_name, condition_array[0][0],
                                            condition_array[0][1], condition_array[0][2])
        correlation = extract_correlation(connection, node.relation_name, condition_array[0][0])
        correlation = correlation[1]
        predicate_symbol = condition_array[0][1]

    print(f"selectivity: {selectivity}")
    print(correlation)

    if not (node.filter):
        actual_selectivity = node.plan_rows / table_tuples
        print(f"actual_selectivity: {actual_selectivity}")

    # 4. Calculate the cost of the index scan using the statistics and selectivity determined.
    index_blocks_selected = index_blocks * selectivity
    index_tuples_selected = index_tuples * selectivity
    table_blocks_selected = table_blocks * selectivity
    table_tuples_selected = table_tuples * selectivity

    # index_access_cost = (index_blocks_selected * random_page_cost) + (index_tuples_selected * cpu_index_tuple_cost) + (index_tuples_selected * cpu_operator_cost)
    index_access_cost = (index_blocks_selected * random_page_cost) + (index_tuples_selected * cpu_index_tuple_cost) + (
                index_tuples_selected * cpu_operator_cost)
    print(f"Index Access Cost: {index_access_cost}")

    index_access_explanation = f"""
        The database performs index scan using {node.index_name} index on the index condition {node.index_cond}.

        The selectivity of the index condition is calculated to be {selectivity}. 

        The cost of index scan is the cost to access the index plus the cost to fetch pages of the table from disk.

        First, the database fetches the corresponding index pages that hold the tuples that match the index condition. The number of index pages fetched is {index_blocks_selected}, which is estimated based on the selectivity.

        The cost for accessing each index page is random, so the cost of fetching the index pages is: no. of index pages selected * random_page_cost

        There is an additional CPU processing cost added for processing the index tuples fetched.

        Next, the database needs to fetch the necessary data blocks. These data blocks are pointed to by the index tuples fetched from the index.

    """

    if correlation >= 0.5:
        # Use best case formula
        table_page_fetch_cost = (table_blocks_selected * seq_page_cost) + (table_tuples_selected * cpu_tuple_cost)
        correlation_explanation = f"""
        Since correlation is {correlation}, which is more than 0.5, we assume that the relation is a clustered relation. 

        This means that the table pages are fetched in sequential order, so the cost is: no. of data blocks retrieved * seq_page_cost

        Additional CPU processing time is added to process all the tuples fetched from the data blocks. 
        """
    else:
        print("Worst case used")
        # Use worst case formula
        table_page_fetch_cost = (table_tuples_selected * random_page_cost) + (table_tuples_selected * cpu_tuple_cost)
        correlation_explanation = f"""
        Since correlation is {correlation}, which is less than 0.5, we assume that the relation is an unclustered relation. 

        This means that the number of pages fetched in the worst case is the number of tuples to be retrieved from the data blocks: no. of tuples to retrieve * random_page_cost

        Additional CPU processing time is added to process all the tuples fetched from the data blocks. 
            """

    print(f"Table Fetch Cost: {table_page_fetch_cost}")

    node.calculated_cost = index_access_cost + table_page_fetch_cost
    # print(f"Index Scan Index Condition: {node.index_cond}")
    # print(f"Index Scan Filter: {node.filter}")
    # print(f"Index Scan Calculated Cost: {node.calculated_cost}")
    # print(f"Index Scan Actual Cost: {node.total_cost}\n")

    if check_accuracy(node.calculated_cost, node.total_cost):
        node.calculation_explain = index_access_explanation + correlation_explanation
    else:
        if correlation < 0.5:
            low_correlation_explain = f"""
        The cost calculated is the worst case scenario, where all the tuples are scattered in different blocks. Thus, the actual cost would probably be less than the cost calculated depending on the correlation value. 

        In addition, the cost calculated assumes that there are no data blocks or table pages cached. In reality, the database caches table pages that are likely to be accessed, so the actual cost to access the data blocks may not be as high as the calculated cost. 
            """
            node.calculation_explain = index_access_explanation + correlation_explanation + low_correlation_explain
        elif predicate_symbol == '<' or predicate_symbol == '>' or predicate_symbol == '<=' or predicate_symbol == '>=':
            inequality_explain = f"""
        Since the index condition has an inequality predicate, we choose 1/3 as the selectivity. This may not be an accurate value because the database may have created a histogram for that table and column, and may have used the histogram to calculate the selectivity.
            """
            node.calculation_explain = index_access_explanation + correlation_explanation + inequality_explain
        else:
            hidden_cost_explain = f"""
        The cost estimation undertaken by the database for index scan is slightly more complicated than what has been explained. There may be more details and operations under the hood that may cause the actual cost to differ vastly from our calculated cost.
            """
            node.calculation_explain = index_access_explanation + correlation_explanation + hidden_cost_explain


def compute_index_only_scan_cost(connection, node):
    # 1. Get necessary statistics and parameter values from the database
    cursor = connection.cursor()

    # Get number of tuples and pages of the table
    table_tuples, table_blocks = get_no_of_tuples_and_blocks(connection, node.relation_name)

    # Get number of tuples and pages of the index
    index_tuples, index_blocks = get_no_of_tuples_and_blocks(connection, node.index_name)

    # Get random_page_cost, cpu_index_tuple_cost, cpu_operator_cost, seq_page_cost and cpu_tuple_cost
    param_query = f"""
                        SELECT name, setting
                        FROM pg_settings
                        WHERE name IN ('seq_page_cost', 'cpu_tuple_cost','cpu_operator_cost','cpu_index_tuple_cost','random_page_cost');
                    """
    cursor.execute(param_query)
    param_result = cursor.fetchall()
    cpu_index_tuple_cost = float(param_result[0][1])
    cpu_operator_cost = float(param_result[1][1])
    cpu_tuple_cost = float(param_result[2][1])
    random_page_cost = float(param_result[3][1])
    seq_page_cost = float(param_result[4][1])

    # Find both selectivity and correlation - if no index condition and if there is index condition
    if not (node.index_cond):
        # If no index conditions, means that it is selecting everything (similar to sequential scan)
        selectivity = 1
        correlation = 1
        predicate_symbol = "="
    else:
        condition_array = parse_condition_string(node.index_cond)
        selectivity = determine_selectivity(connection, node.relation_name, condition_array[0][0],
                                            condition_array[0][1], condition_array[0][2])
        correlation = extract_correlation(connection, node.relation_name, condition_array[0][0])
        correlation = correlation[1]
        predicate_symbol = condition_array[0][1]

    print(f"selectivity: {selectivity}")
    print(correlation)

    if not (node.filter):
        actual_selectivity = node.plan_rows / table_tuples
        print(f"actual_selectivity: {actual_selectivity}")

    relallvisible = extract_relallvisible(connection, node.relation_name)
    fraction_visible = relallvisible / table_tuples

    # 4. Calculate the cost of the index scan using the statistics and selectivity determined.
    index_blocks_selected = index_blocks * selectivity
    index_tuples_selected = index_tuples * selectivity
    table_blocks_selected = table_blocks * selectivity
    table_tuples_selected = table_tuples * selectivity

    index_access_cost = (index_blocks_selected * random_page_cost) + (index_tuples_selected * cpu_index_tuple_cost) + (
            index_tuples_selected * cpu_operator_cost)
    print(f"Index Access Cost: {index_access_cost}")

    index_access_explanation = f"""
        The database performs index scan using {node.index_name} index on the index condition {node.index_cond}.

        The selectivity of the index condition is calculated to be {selectivity}. 

        The cost of index scan is the cost to access the index plus the cost to fetch pages of the table from disk.

        First, the database fetches the corresponding index pages that hold the tuples that match the index condition. The number of index pages fetched is {index_blocks_selected}, which is estimated based on the selectivity.

        The cost for accessing each index page is random, so the cost of fetching the index pages is: no. of index pages selected * random_page_cost

        There is an additional CPU processing cost added for processing the index tuples fetched.

        Next, the database needs to fetch the necessary data blocks. These data blocks are pointed to by the index tuples fetched from the index.

        """

    if correlation >= 0.5:
        # Use best case formula
        table_page_fetch_cost = (1 - fraction_visible)(table_blocks_selected * seq_page_cost) + (
                    table_tuples_selected * cpu_tuple_cost)
        correlation_explanation = f"""
        Since correlation is {correlation}, which is more than 0.5, we assume that the relation is a clustered relation. 

        For index only scan, only the visible tuples are fetched from the disk, which are indicated by the visibility map data structure kept by the database.

        This means that the table pages are fetched in sequential order, so the cost is: no. of data blocks retrieved * seq_page_cost

        Additional CPU processing time is added to process all the tuples fetched from the data blocks. 
        """
    else:
        print("Worst case used")
        # Use worst case formula
        table_page_fetch_cost = (1 - fraction_visible)(table_tuples_selected * random_page_cost) + (
                    table_tuples_selected * cpu_tuple_cost)
        correlation_explanation = f"""
        Since correlation is {correlation}, which is less than 0.5, we assume that the relation is an unclustered relation. 

        For index only scan, only the visible tuples are fetched from the disk, which are indicated by the visibility map data structure kept by the database.

        This means that the number of pages fetched in the worst case is the number of tuples to be retrieved from the data blocks: no. of tuples to retrieve * random_page_cost

        Additional CPU processing time is added to process all the tuples fetched from the data blocks. 
        """

    node.calculated_cost = index_access_cost + table_page_fetch_cost

    if check_accuracy(node.calculated_cost, node.total_cost):
        node.calculation_explain = index_access_explanation + correlation_explanation
    else:
        if correlation < 0.5:
            low_correlation_explain = f"""
        The cost calculated is the worst case scenario, where all the tuples are scattered in different blocks. Thus, the actual cost would probably be less than the cost calculated depending on the correlation value. 

        In addition, the cost calculated assumes that there are no data blocks or table pages cached. In reality, the database caches table pages that are likely to be accessed, so the actual cost to access the data blocks may not be as high as the calculated cost. 
            """
            node.calculation_explain = index_access_explanation + correlation_explanation + low_correlation_explain
        elif predicate_symbol == '<' or predicate_symbol == '>' or predicate_symbol == '<=' or predicate_symbol == '>=':
            inequality_explain = f"""
        Since the index condition has an inequality predicate, we choose 1/3 as the selectivity. This may not be an accurate value because the database may have created a histogram for that table and column, and may have used the histogram to calculate the selectivity.
            """
            node.calculation_explain = index_access_explanation + correlation_explanation + inequality_explain
        else:
            hidden_cost_explain = f"""
        The cost estimation undertaken by the database for index scan is slightly more complicated than what has been explained. There may be more details and operations under the hood that may cause the actual cost to differ vastly from our calculated cost.
            """
            node.calculation_explain = index_access_explanation + correlation_explanation + hidden_cost_explain


def extract_relallvisible(connection, relation_name):
    cursor = connection.cursor()
    query = """
        SELECT relallvisible
        FROM pg_class
        WHERE relname = %s;
        """
    cursor.execute(query, (relation_name,))
    results = cursor.fetchone()

    return results


def parse_condition_string(condition_string):
    """
    Returns an array of attributes and values for a given index cond string in QEP output of index scan
    :param index_cond:
    :return:
    """

    # Regular expression pattern to match conditions
    pattern = r'\((\w+)\s*([><=]+)\s*\'?([^:]+?)\'?(?::[^)]+)?\)'

    # Find all conditions in the input string
    conditions = re.findall(pattern, condition_string)

    # List to store extracted attribute names, operators, and values
    attribute_value_pairs = []

    # Iterate through each condition
    for condition in conditions:
        attribute_value_pairs.append(condition)

    return attribute_value_pairs


def get_histogram_bounds(connection, relation_name, attr_name):
    cursor = connection.cursor()

    query = """
        SELECT histogram_bounds
        FROM pg_stats
        WHERE tablename = %s 
        AND attname = %s;
        """

    cursor.execute(query, (relation_name, attr_name))
    results = cursor.fetchone()
    if results == None:
        return results
    else:
        buckets = results[0][1:-1].split(',')

        # Convert each string element to a float
        # array = [element for element in elements]
        bucket_array = [bucket for bucket in buckets]

        return bucket_array


def get_mcv_mcf(connection, relation_name, attr_name):
    cursor = connection.cursor()

    query = """
        SELECT most_common_vals, most_common_freqs
        FROM pg_stats
        WHERE tablename = %s 
        AND attname = %s;
        """

    cursor.execute(query, (relation_name, attr_name))
    results = cursor.fetchone()

    if (results[0] == None) and (results[1] == None):
        return results

    # Remove the curly braces and split the string
    elements = results[0][1:-1].split(',')

    # Convert each string element to a float
    array = [element for element in elements]

    result_array = [array, results[1]]

    return result_array


def get_freq_info(connection, relation_name, attr_name):
    cursor = connection.cursor()

    query = """
    SELECT n_distinct, null_frac
    FROM pg_stats
    WHERE tablename = %s 
    AND attname = %s;
    """

    cursor.execute(query, (relation_name, attr_name))
    results = cursor.fetchone()

    return results


def check_mcv(mcv, string):
    """
    Return -1 if no match found, return index if found
    :param mcv:
    :param string:
    :return:
    """
    if mcv == None:
        return -1

    for count, i in enumerate(mcv):
        if i == str(string):
            return count

    return -1


def determine_selectivity(connection, relation_name, condition_attribute, predicate_symbol, condition_value):
    """
    Given a condition and relation, find the selectivity
    :param connection:
    :param relation_name:
    :param condition_attribute:
    :param condition_value:
    :param predicate_symbol:
    :return:
    """
    n_distinct, null_frac = get_freq_info(connection, relation_name, condition_attribute)
    histogram_bounds = get_histogram_bounds(connection, relation_name, condition_attribute)
    mcv, mcf = get_mcv_mcf(connection, relation_name, condition_attribute)

    table_tuples, table_blocks = get_no_of_tuples_and_blocks(connection, relation_name)

    expression_pattern = r'\b\w+\.\w+\b'  # Matches "table_name.column_name" pattern

    expressions = re.findall(expression_pattern, condition_value)

    if predicate_symbol == '=':
        mcf_index = check_mcv(mcv, condition_value)
        if mcf_index != -1:
            print("MCF used")
            return mcf[mcf_index]
        elif histogram_bounds != None and expressions == []:
            print("Histogram used")
            if mcf != None:
                sum_mcf = sum(mcf)
            else:
                sum_mcf = 0
            num_buckets = len(histogram_bounds)
            selectivity = (1 - null_frac - sum_mcf) / num_buckets
            return selectivity
        elif n_distinct < 0:
            selectivity = abs(n_distinct) / table_tuples
        else:
            selectivity = 1 / n_distinct
        print("n_distinct_values used")
        return selectivity
    else:
        selectivity = 1 / 3
        return selectivity


def extract_correlation(connection, relation_name, attr_name):
    cursor = connection.cursor()
    query = """
    SELECT attname, correlation
    FROM pg_stats
    WHERE tablename = %s AND attname = %s;
    """
    cursor.execute(query, (relation_name, attr_name))
    results = cursor.fetchone()

    return results


def compute_bitmap_index_scan_cost(connection, node):
    # 1. Get necessary statistics and parameter values from the database
    cursor = connection.cursor()

    # Get number of tuples and pages of the table
    relation_name = node.parent.relation_name
    table_tuples, table_blocks = get_no_of_tuples_and_blocks(connection, relation_name)

    # Get number of tuples and pages of the index
    index_tuples, index_blocks = get_no_of_tuples_and_blocks(connection, node.index_name)

    # Get random_page_cost, cpu_index_tuple_cost, cpu_operator_cost, seq_page_cost and cpu_tuple_cost
    param_query = f"""
                        SELECT name, setting
                        FROM pg_settings
                        WHERE name IN ('seq_page_cost', 'cpu_tuple_cost','cpu_operator_cost','cpu_index_tuple_cost','random_page_cost');
                    """
    cursor.execute(param_query)
    param_result = cursor.fetchall()
    cpu_index_tuple_cost = float(param_result[0][1])
    cpu_operator_cost = float(param_result[1][1])
    cpu_tuple_cost = float(param_result[2][1])
    random_page_cost = float(param_result[3][1])
    seq_page_cost = float(param_result[4][1])

    selectivity = node.plan_rows / table_tuples
    index_blocks_selected = index_blocks * selectivity
    index_tuples_selected = index_tuples * selectivity

    node.calculated_cost = random_page_cost * index_blocks_selected + cpu_index_tuple_cost * index_tuples_selected + cpu_operator_cost * index_tuples_selected

    print("Actual cost for node " + node.node_type + ": " + str(node.total_cost))
    print("Calculated cost for node " + node.node_type + ": " + str(node.calculated_cost))


def compute_bitmap_heap_scan_cost(connection, node):
    # 1. Get necessary statistics and parameter values from the database
    cursor = connection.cursor()

    # Get number of tuples and pages of the table
    table_tuples, table_blocks = get_no_of_tuples_and_blocks(connection, node.relation_name)

    # Get number of tuples and pages of the index
    index_name = node.children[0].index_name
    index_cond = node.children[0].index_cond
    index_tuples, index_blocks = get_no_of_tuples_and_blocks(connection, index_name)

    # Get number of plan rows returned by bitmap_index_scan
    plan_rows = node.children[0].plan_rows
    selectivity = plan_rows / index_tuples
    bitmap_index_scan_cost = node.children[0].total_cost

    # Get random_page_cost, cpu_index_tuple_cost, cpu_operator_cost, seq_page_cost and cpu_tuple_cost
    param_query = f"""
                            SELECT name, setting
                            FROM pg_settings
                            WHERE name IN ('seq_page_cost', 'cpu_tuple_cost','cpu_operator_cost','cpu_index_tuple_cost','random_page_cost');
                        """
    cursor.execute(param_query)
    param_result = cursor.fetchall()
    cpu_index_tuple_cost = float(param_result[0][1])
    cpu_operator_cost = float(param_result[1][1])
    cpu_tuple_cost = float(param_result[2][1])
    random_page_cost = float(param_result[3][1])
    seq_page_cost = float(param_result[4][1])

    pages_fetched = min((2 * table_blocks * table_tuples * selectivity / 2 * table_blocks + table_tuples * selectivity),
                        table_blocks)

    tuples_fetched = table_tuples * selectivity

    cost_per_page = (random_page_cost) - (random_page_cost - seq_page_cost) * math.sqrt(pages_fetched / table_blocks)

    startup_cost = bitmap_index_scan_cost + 0.1 * cpu_operator_cost * table_tuples * selectivity

    run_cost = pages_fetched * cost_per_page + tuples_fetched * cpu_tuple_cost + tuples_fetched * cpu_operator_cost

    node.calculated_cost = startup_cost + run_cost

    print("Actual cost for node " + node.node_type + ": " + str(node.total_cost))
    print("Calculated cost for node " + node.node_type + ": " + str(node.calculated_cost))

    if check_accuracy(node.calculated_cost, node.total_cost):
        node.calculation_explain = f"""
        The database performs a bitmap heap scan on {node.relation_name} with index condition {index_cond}. 

        It uses a bitmap index scan to fetch corresponding rows from the heap. Calculates the selectivity by number of rows {plan_rows} returned by bitmap index to number of rows {index_tuples} in index {node.index_name}.

        Based on selectivity of the query, it estimates the number of pages to be fetched {pages_fetched} from the heap and estimates the number of tuples to be fetched {tuples_fetched} from the heap.

        It then estimates the cost of fetching each page relative to total number pages in relation using random page cost {random_page_cost} and sequential page cost {seq_page_cost}.

        Bitmap heap scan cost takes into account CPU processing time from the bitmap index scan, cost of fetching the {pages_fetched} pages and processing the estimated number of tuples using CPU tuple cost {cpu_tuple_cost} and CPU operator cost {cpu_operator_cost}.
"""
    else:
        node.calculation_explain = f"""
        The database performs a bitmap heap scan on {node.relation_name} with index condition {index_cond}. 

        It uses a bitmap index scan to fetch corresponding rows from the heap. Calculates the selectivity by number of rows {plan_rows} returned by bitmap index to number of rows {index_tuples} in index {node.index_name}.

        We have assumed independence here but PostgreSQL takes into account correlation between columns when calculating selectivity.

        Based on selectivity of the query, it estimates the number of pages to be fetched {pages_fetched} from the heap and estimates the number of tuples to be fetched {tuples_fetched} from the heap.

        It then estimates the cost of fetching each page relative to total number pages in relation using random page cost {random_page_cost} and sequential page cost {seq_page_cost}.

        Bitmap heap scan cost takes into account CPU processing time from the bitmap index scan, cost of fetching the {pages_fetched} pages and processing the estimated number of tuples using CPU tuple cost {cpu_tuple_cost} and CPU operator cost {cpu_operator_cost}.

        Our formula assumed the multiplier of 0.1 to scale the CPU operator cost in the calculation of startup cost which could be different from PostgreSQL’s internal calculations. Thus leading to differing results. Additionally, disk speed and caching can affect the actual cost of fetching pages too.
        """


# Cost Estimation of join nodes
def compute_nested_loop_cost(connection, node):
    cursor = connection.cursor()
    # Get cpu_tuple_cost
    param_query = f"""
                                SELECT name, setting
                                FROM pg_settings
                                WHERE name IN ('cpu_tuple_cost');
                            """
    cursor.execute(param_query)
    param_result = cursor.fetchall()
    cpu_tuple_cost = float(param_result[0][1])
    print(cpu_tuple_cost)

    # Get the cost of both outer and inner child nodes
    outer_startup_cost = node.children[0].startup_cost
    inner_startup_cost = node.children[1].startup_cost
    outer_total_cost = node.children[0].total_cost
    inner_total_cost = node.children[1].total_cost
    outer_rows = node.children[0].plan_rows
    inner_rows = node.children[1].plan_rows

    # If either of the child nodes are Materialize nodes
    for child in node.children:
        if child.node_type == 'Materialize':
            node.calculated_cost = outer_total_cost + inner_total_cost + (inner_rows - 1) * 0.0125 + (
                        (outer_rows * inner_rows) * cpu_tuple_cost)
            print("Actual cost for node " + node.node_type + ": " + str(node.total_cost))
            print("Calculated cost for node " + node.node_type + ": " + str(node.calculated_cost))

            if check_accuracy(node.calculated_cost, node.total_cost):
                node.calculation_explain = f"""
        The database performs a nested loop join on table <relation_name> and <relation_name>.

        An assumed multiplier of 0.125 is associated with each tuple from outer relation for the materialise node call.

        Nested loop cost takes the sum of total cost of outer and inner node, cost from materialise node call and CPU processing cost for each output row of the join.
                """
            else:
                node.calculation_explain = f"""
        The database performs a nested loop join on table <relation_name> and <relation_name> .

        An assumed multiplier of 0.125 is associated with each tuple from outer relation for the materialise node call. This multiplier is an assumption and should be fine tuned for alignment with actual cost.

        Nested loop cost takes the sum of total cost of outer and inner node, cost from materialise node call and CPU processing cost for each output row of the join.
                """

            return

    # If no Materialize as child nodes
    node.calculated_cost = outer_total_cost + (outer_rows * inner_total_cost) + (node.plan_rows * cpu_tuple_cost)

    print("Actual cost for node " + node.node_type + ": " + str(node.total_cost))
    print("Calculated cost for node " + node.node_type + ": " + str(node.calculated_cost))

    if check_accuracy(node.calculated_cost, node.total_cost):
        node.calculation_explain = f"""
        The database performs a nested loop join on table <relation_name> and <relation_name> 

        It takes the sum of total cost of the outer node, the cumulative cost of executing the inner relation for each row in the outer relation and the total CPU cost of processing the result rows of the join.

        Cost: ( total cost in outer node) + (no_of_tuples in outer node * total cost in inner node) + (no_of_tuples in outer relation * no_of_tuples in inner relation * cpu_tuple_cost)  
        """
    else:
        node.calculation_explain = f"""
        The database performs a nested loop join on table <relation_name> and <relation_name> 

        It takes the sum of total cost of the outer node, the cumulative cost of executing the inner relation for each row in the outer relation and the total CPU cost of processing the result rows of the join.

        While our algorithm marks a general case to estimate the cost, there are a few factors that could further improve the accuracy of cost estimation.

        1. Including join selectivity in the calculation allows us to estimate the number of rows in output more accurately. Done by using pg statistics in the database.
        2. In complex queries, there could be additional processing involved in evaluating expressions, filtering and projections which will increase CPU usage and hence cost.
        3. I/O costs based on the number of blocks accessed and cost of accessing each block and cost of fetching data from disk such as reading from tables, indexes can contribute significantly to cost, especially if relations involved in join do not fit in memory.
        """


def compute_hash_join_cost(connection, node):
    cursor = connection.cursor()
    # Get random_page_cost, cpu_index_tuple_cost, cpu_operator_cost, seq_page_cost and cpu_tuple_cost
    param_query = f"""
                        SELECT name, setting
                        FROM pg_settings
                        WHERE name IN ('seq_page_cost', 'cpu_tuple_cost','cpu_operator_cost','cpu_index_tuple_cost','random_page_cost');
                    """
    cursor.execute(param_query)
    param_result = cursor.fetchall()
    cpu_index_tuple_cost = float(param_result[0][1])
    cpu_operator_cost = float(param_result[1][1])
    cpu_tuple_cost = float(param_result[2][1])
    random_page_cost = float(param_result[3][1])
    seq_page_cost = float(param_result[4][1])

    # Compute selectivity
    outer_rows = node.children[0].plan_rows
    inner_rows = node.children[1].plan_rows
    selectivity = node.plan_rows / (outer_rows * inner_rows)
    outer_cost = node.children[0].total_cost
    inner_cost = node.children[1].total_cost

    print(inner_cost)

    # Calculate hash join cost
    node.calculated_cost = inner_cost + outer_cost + (inner_rows * cpu_operator_cost) + (
                outer_rows * cpu_operator_cost) + (cpu_operator_cost * inner_rows * outer_rows * selectivity) + (
                                       cpu_tuple_cost * node.plan_rows)

    print("Actual cost for node " + node.node_type + ": " + str(node.total_cost))
    print("Calculated cost for node " + node.node_type + ": " + str(node.calculated_cost))

    if check_accuracy(node.calculated_cost, node.total_cost):
        node.calculation_explain = f"""
        The database performs a Hash on table <relation_name> and performs a Hash join on table <relation_name> under condition {node.hash_cond}.

        It takes the cost of Hash node which is the cost to build the hash table from inner relation by reading the tuples {inner_rows} Then it takes the CPU cost of processing these tuples by taking {cpu_operator_cost} and {cpu_tuple_cost}.

        It then takes the cost of processing the outer node. This scans the outer relation and uses each tuple to probe hash tables to find matches. Then the CPU processes each tuple in the inner node and outer node using cpu operator cost {cpu_operator_cost}.

        Next, it includes the join matching cost to match tuples from the outer node against the hash table. It involves the processing of each combination of tuples with cpu operator cost from outer and inner nodes multiplied with a selectivity of the join. Here we have taken selectivity to be an estimated number of resulting rows from join as a proportion of the total possible combination of tuples from outer and inner nodes.

        Total cost will involve the above and the cost of processing resultant rows from join operation using cpu cost of processing each tuple. 

        """
    else:
        node.calculation_explain = f"""
        The database performs a Hash on table <relation_name> and performs a Hash join on table <relation_name> under condition {node.hash_cond}.

        It takes the cost of Hash node which is the cost to build the hash table from inner relation by reading the tuples {inner_rows} Then it takes the CPU cost of processing these tuples by taking {cpu_operator_cost} and {cpu_tuple_cost}.

        It then takes the cost of processing the outer node. This scans the outer relation and uses each tuple to probe hash tables to find matches. Then the CPU processes each tuple in the inner node and outer node using cpu operator cost {cpu_operator_cost}.

        Next, it includes the join matching cost to match tuples from the outer node against the hash table. It involves the processing of each combination of tuples with cpu operator cost from outer and inner nodes multiplied with a selectivity of the join. Here we have taken selectivity to be an estimated number of resulting rows from join as a proportion of the total possible combination of tuples from outer and inner nodes.

        The differences of cost can arise from the selectivity calculation. A better estimate of selectivity of a hash join can be done by calculating the cardinality (number of distinct values) of the columns involved in the join condition. This is retrieved from the catalog ‘pg_statistic’ table as the ‘stadistinct’ value for each column. Selectivity can then be estimated as the inverse of the maximum cardinality of the join columns.
        Additionally, there could be more adjustments to the selectivity estimate based on distribution and correlation of the data in the columns. This accounts for non-uniform distributions and dependencies between columns.

        Total cost will involve the above and the cost of processing resultant rows from join operation using cpu cost of processing each tuple. 
        """


def compute_hash_semi_join_cost(connection, node):
    cursor = connection.cursor()
    # Get random_page_cost, cpu_index_tuple_cost, cpu_operator_cost, seq_page_cost and cpu_tuple_cost
    param_query = f"""
                        SELECT name, setting
                        FROM pg_settings
                        WHERE name IN ('seq_page_cost', 'cpu_tuple_cost','cpu_operator_cost','cpu_index_tuple_cost','random_page_cost');
                    """
    cursor.execute(param_query)
    param_result = cursor.fetchall()
    cpu_index_tuple_cost = float(param_result[0][1])
    cpu_operator_cost = float(param_result[1][1])
    cpu_tuple_cost = float(param_result[2][1])
    random_page_cost = float(param_result[3][1])
    seq_page_cost = float(param_result[4][1])

    # Compute selectivity
    outer_rows = node.children[0].plan_rows
    inner_rows = node.children[1].plan_rows
    selectivity = node.plan_rows / (outer_rows * inner_rows)
    outer_cost = node.children[0].total_cost
    inner_cost = node.children[1].total_cost

    print(inner_cost)

    # Calculate hash join cost
    node.calculated_cost = inner_cost + outer_cost + (inner_rows * cpu_operator_cost) + (
                outer_rows * cpu_operator_cost) + (cpu_operator_cost * inner_rows * outer_rows * selectivity) + (
                                       cpu_tuple_cost * node.plan_rows)

    print("Actual cost for node " + node.node_type + ": " + str(node.total_cost))
    print("Calculated cost for node " + node.node_type + ": " + str(node.calculated_cost))

    if check_accuracy(node.calculated_cost, node.total_cost):
        node.calculation_explain = f"""
        The database performs a Hash on table <relation_name> and performs a Hash join on table <relation_name> under condition {node.hash_cond}.

        It takes the cost of Hash node which is the cost to build the hash table from inner relation by reading the tuples {inner_rows} Then it takes the CPU cost of processing these tuples by taking {cpu_operator_cost} and {cpu_tuple_cost}.

        It then takes the cost of processing the outer node. This scans the outer relation and uses each tuple to probe hash tables to find matches. Then the CPU processes each tuple in the inner node and outer node using cpu operator cost {cpu_operator_cost}.

        Next, it includes the join matching cost to match tuples from the outer node against the hash table. It involves the processing of each combination of tuples with cpu operator cost from outer and inner nodes multiplied with a selectivity of the join. Here we have taken selectivity to be an estimated number of resulting rows from join as a proportion of the total possible combination of tuples from outer and inner nodes.

        Total cost will involve the above and the cost of processing resultant rows from join operation using cpu cost of processing each tuple. 
        """
    else:
        node.calculation_explain = f"""
        The database performs a Hash on table <relation_name> and performs a Hash join on table <relation_name> under condition {node.hash_cond}.

        It takes the cost of Hash node which is the cost to build the hash table from inner relation by reading the tuples {inner_rows} Then it takes the CPU cost of processing these tuples by taking {cpu_operator_cost} and {cpu_tuple_cost}.

        In the case of a Hash Semi Join, the cost estimation differs from the classic hash join. In a classic hash join, all matching tuples from both the outer and inner node are combined to form resulting rows. However, a hash semi join only returns each matching record from the outer node only once, regardless of how many matches exist in the inner relation. 

        Subsequently, the estimated cost of processing resulting rows will be different as well as it only involves the distinct tuples from the outer node.

        In a Hash Semi Join, the selectivity may be estimated as a measure of the probability that a tuple from the outer node will find a match in the inner node based on the join condition. These distinct values can be found using stadistinct. Can be calculated as the ratio of distinct values in the outer node’ join column to the minimum distinct values in join columns of both relations.
        """


def compute_merge_join_cost(connection, node):
    # Get the number of tuples and cost of both child nodes
    inner_rows = node.children[1].plan_rows
    inner_cost = node.children[1].total_cost
    outer_rows = node.children[0].plan_rows
    outer_cost = node.children[0].total_cost

    cursor = connection.cursor()
    # Get random_page_cost, cpu_index_tuple_cost, cpu_operator_cost, seq_page_cost and cpu_tuple_cost
    param_query = f"""
                            SELECT name, setting
                            FROM pg_settings
                            WHERE name IN ('seq_page_cost', 'cpu_tuple_cost','cpu_operator_cost','cpu_index_tuple_cost','random_page_cost');
                        """
    cursor.execute(param_query)
    param_result = cursor.fetchall()
    cpu_index_tuple_cost = float(param_result[0][1])
    cpu_operator_cost = float(param_result[1][1])
    cpu_tuple_cost = float(param_result[2][1])
    random_page_cost = float(param_result[3][1])
    seq_page_cost = float(param_result[4][1])

    node.calculated_cost = outer_cost + inner_cost + (cpu_tuple_cost * inner_rows) + (
                cpu_operator_cost * (inner_rows + outer_rows))

    print("Actual cost for node " + node.node_type + ": " + str(node.total_cost))
    print("Calculated cost for node " + node.node_type + ": " + str(node.calculated_cost))

    node.calculation_explain = f"""
        The database does a merge join on <relation_name1> and <relation_name2>.

        It scans all <no_of_tuples> tuples of <relation_name1>  and <relation_name2> one by one. Then, the CPU processes the two relations to join rows that satisfy the given condition. 

        Cost: cost of scanning index node (of A) + cost of scanning index node (of B) + cpu_tuple_cost*(no_of_tuples_A) + cpu_operator_cost*(no_of_tuples_A + no_of_tuples_B).
    """


# Cost Estimation of Non-scan and Non-join nodes
def compute_hash_cost(node):
    node.calculated_cost = node.children[0].total_cost
    node.calculation_explain = "The cost of hash node is the same as its child node."

    print("Actual cost for node " + node.node_type + ": " + str(node.total_cost))
    print("Calculated cost for node " + node.node_type + ": " + str(node.calculated_cost))

    node.calculation_explain = f"""
        In a Hash node, the child node, usually the inner relation is scanned and processed to construct a hash table, the hash node itself does not add significant additional cost as its goal is to manage the hash table. Hence the cost of this step is primarily represented by the child node’s cost.
    """


def compute_sort_cost(connection, node):
    # First, check whether the intermediate table or table fits in the working memory
    child_node = node.children[0]
    cursor = connection.cursor()
    # Get random_page_cost, cpu_index_tuple_cost, cpu_operator_cost, seq_page_cost and cpu_tuple_cost
    param_query = f"""
                            SELECT name, setting
                            FROM pg_settings
                            WHERE name IN ('block_size','work_mem', 'cpu_operator_cost','seq_page_cost','random_page_cost');
                        """
    cursor.execute(param_query)
    param_result = cursor.fetchall()
    block_size = float(param_result[0][1])
    cpu_operator_cost = float(param_result[1][1])
    random_page_cost = float(param_result[2][1])
    seq_page_cost = float(param_result[3][1])
    work_mem = float(param_result[4][1])

    # Find the number of tuples that fit in a block
    max_tuples_per_block = math.floor(block_size / node.plan_width)
    num_blocks = math.ceil(node.plan_rows / max_tuples_per_block)
    work_blocks = work_mem / (block_size / 1024)  # work_mem is in KB, but block_size is in bytes
    # table_memory = (node.plan_rows * node.plan_width) / 1024
    print(max_tuples_per_block)
    print(num_blocks)
    print(work_blocks)

    if num_blocks <= work_blocks:
        # Do in-memory quicksort
        node.calculated_cost = child_node.total_cost + (
                    2 * cpu_operator_cost * node.plan_rows * math.log(node.plan_rows, 2)) + (
                                           cpu_operator_cost * node.plan_rows)
        print("In-memory sort")
        print("Actual cost for node " + node.node_type + ": " + str(node.total_cost))
        print("Calculated cost for node " + node.node_type + ": " + str(node.calculated_cost))

        if check_accuracy(node.calculated_cost, node.total_cost):
            node.calculation_explain = f"""
        The quicksort algorithm is used when data sorted fits into allowed memory {work_mem} completely. 

        The startup sorting cost involves the total cost of the child node and comparison operations cost. A multiplier of 2 is applied to cpu operator cost to suggest that comparisons during sorting are more expensive than other CPU operations. Since the complexity of sorting n values is O (n log n) with base of 2. This is implemented in our calculation.

        It then processes each row of sort node by taking cpu operator cost for each resultant row {node.plan_rows}

        Quicksort cost estimation would be the sum of startup sorting cost with the processing cost of each resultant row of sort node.
            """
        else:
            node.calculation_explain = f"""
        The quicksort algorithm is used when data sorted fits into allowed memory {work_mem} completely. 

        The startup sorting cost involves the total cost of the child node and comparison operations cost. A multiplier of 2 is applied to cpu operator cost to suggest that comparisons during sorting are more expensive than other CPU operations. Since the complexity of sorting n values is O (n log n) with base of 2. This is implemented in our calculation.

        This multiplier is an assumption for simplification of cost estimation. However, it may not be universally true for all systems and scenarios. It may be subjected to finetuning based on actual performance testing and profiling of your specific database system and environment.

        It then processes each row of sort node by taking cpu operator cost for each resultant row {node.plan_rows}

        Quicksort cost estimation would be the sum of startup sorting cost with the processing cost of each resultant row of sort node.

                """
    else:
        # Do external merge sort
        node.calculated_cost = child_node.total_cost + (
                    2 * cpu_operator_cost * node.plan_rows * math.log(node.plan_rows, 2)) + (
                                           seq_page_cost * 0.75 + random_page_cost * 0.25) * 2 * num_blocks * 1 + (
                                           cpu_operator_cost * node.plan_rows)
        print("External merge sort")
        print("Actual cost for node " + node.node_type + ": " + str(node.total_cost))
        print("Calculated cost for node " + node.node_type + ": " + str(node.calculated_cost))

        if check_accuracy(node.calculated_cost, node.total_cost):
            node.calculation_explain = f"""
        The external merge sort algorithm is used if data to be read is too big to be sorted in memory, sort node changes to external merge sort method. Rows are scanned in batches of pages instead of one by one to save on I/O costs. 

        It takes the startup sorting cost similar to quick sort where it involves the total cost of the child node and comparison operations cost. A multiplier of 2 is applied to cpu operator cost to suggest that comparisons during sorting are more expensive than other CPU operations. Since the complexity of sorting n values is O (n log n) with base of 2. This is implemented in our calculation.

        It then processes each row of sort node by taking cpu operator cost for each resultant row {node.plan.rows}

        External merge sort takes into account the estimated cost of performing I/O operations for reading and writing during the sort. Our calculation involves the mix of sequential and random page access costs assumed to 75% and 25% respectively. This is multiplied by the number of blocks of relation since merge sort involves read and write at least twice.

        Cost: 
        cost of child node (e.g. seq scan etc) + cpu_operator_cost*no_of_tuples_scanned* 2 *log 2n+ (seq_page_cost * 0.75 + random_page_cost*0.25) *2 * number_of_blocks * number of loops
            """
        else:
            node.calculation_explain = f"""
        The external merge sort algorithm is used if data to be read is too big to be sorted in memory, sort node changes to external merge sort method. Rows are scanned in batches of pages instead of one by one to save on I/O costs. 

        It takes the startup sorting cost similar to quick sort where it involves the total cost of the child node and comparison operations cost. A multiplier of 2 is applied to cpu operator cost to suggest that comparisons during sorting are more expensive than other CPU operations. Since the complexity of sorting n values is O (n log n) with base of 2. This is implemented in our calculation.

        It then processes each row of sort node by taking cpu operator cost for each resultant row {node.plan.rows}

        External merge sort takes into account the estimated cost of performing I/O operations for reading and writing during the sort. Our calculation involves the mix of sequential and random page access costs assumed to 75% and 25% respectively. This is multiplied by the number of blocks of relation since merge sort involves read and write at least twice.

        In this estimation of costs, there are assumptions of multiplier of 2 for cpu operator cost, the 75% sequential and 25% random page access costs which may not reflect the actual cost in postgresql environment.

        Factors including disk speed and data distribution, size and skew can contribute to the discrepancy in cost estimation. Additionally, there could be overhead costs that are not accounted for such as concurrency control or buffer management.
            """


# TODO: Code out compute aggregate cost
def compute_aggregate_cost(connection, node):
    cursor = connection.cursor()
    # Get random_page_cost, cpu_index_tuple_cost, cpu_operator_cost, seq_page_cost and cpu_tuple_cost
    param_query = f"""
                                SELECT name, setting
                                FROM pg_settings
                                WHERE name IN ('seq_page_cost', 'cpu_tuple_cost','cpu_operator_cost','cpu_index_tuple_cost','random_page_cost');
                            """
    cursor.execute(param_query)
    param_result = cursor.fetchall()
    cpu_index_tuple_cost = float(param_result[0][1])
    cpu_operator_cost = float(param_result[1][1])
    cpu_tuple_cost = float(param_result[2][1])
    random_page_cost = float(param_result[3][1])
    seq_page_cost = float(param_result[4][1])

    child_cost = node.children[0].total_cost
    child_rows = node.children[0].plan_rows

    node.calculated_cost = child_cost + (child_rows * cpu_operator_cost) + (node.plan_rows * cpu_tuple_cost)

    print("Actual cost for node " + node.node_type + ": " + str(node.total_cost))
    print("Calculated cost for node " + node.node_type + ": " + str(node.calculated_cost))

    node.calculation_explain = f"""
        The database does the aggregation by first estimating the cost of the intermediate table which is {child_cost}, by scanning the child node.

        Then, it scans all {node.plan_rows} rows, which is the estimated number of rows processed, and the CPU processes all of the rows. 

        Cost: (cost_of_child_node) + (estimated rows processed * cpu_operator_cost) + (estimated rows returned * cpu_tuple_cost)
            """


def compute_materialize_cost(connection, node):
    child_cost = node.children[0].total_cost
    node.calculated_cost = child_cost + 0.03

    print("Actual cost for node " + node.node_type + ": " + str(node.total_cost))
    print("Calculated cost for node " + node.node_type + ": " + str(node.calculated_cost))

    node.calculation_explain = f"""
        The Materialize node materialises all the tuples from its child node. 

        Cost: cost of child node + overhead cost of 0.03
"""


def compute_memoize_cost(connection, node):
    cursor = connection.cursor()
    # Get random_page_cost, cpu_index_tuple_cost, cpu_operator_cost, seq_page_cost and cpu_tuple_cost
    param_query = f"""
                                    SELECT name, setting
                                    FROM pg_settings
                                    WHERE name IN ('cpu_tuple_cost');
                                """
    cursor.execute(param_query)
    param_result = cursor.fetchall()
    cpu_tuple_cost = float(param_result[0][1])

    child_cost = node.children[0].total_cost
    node.calculated_cost = child_cost + cpu_tuple_cost

    print("Actual cost for node " + node.node_type + ": " + str(node.total_cost))
    print("Calculated cost for node " + node.node_type + ": " + str(node.calculated_cost))

    node.calculation_explain = f"""
        The Memoize node gets all tuples from its child node and caches them if they are not in cache. 

        Cost: cost of child node + cpu_tuple_cost
    """


def check_accuracy(calculated_cost, actual_cost):
    # Calculate the threshold for 20% of the actual cost
    threshold = actual_cost * 0.2

    # Check if the absolute difference between the calculated cost
    # and the actual cost is within the threshold
    if abs(calculated_cost - actual_cost) <= threshold:
        return True
    else:
        return False


def get_no_of_tuples_and_blocks(connection, table_name):
    """
    Gets number of blocks and tuples of a table from the database
    :param connection:
    :param table_name:
    :return:
    """
    cursor = connection.cursor()

    # Get number of tuples and pages of the table
    rel_query = f"""
                        SELECT reltuples, relpages
                        FROM pg_class
                        WHERE relname = %s;
                    """
    cursor.execute(rel_query, (table_name,))
    result = cursor.fetchone()
    table_tuples = result[0]
    table_blocks = result[1]
    return table_tuples, table_blocks


# Establish a connection to the database
conn = psycopg2.connect(
    dbname="TPC-H",
    user="postgres",
    password="root",
    host="localhost",
    port="5432"
)

# Create a cursor object to execute SQL queries
cur = conn.cursor()

# Disable parallel query execution
query = """
SET max_parallel_workers_per_gather = 0;
"""

cur.execute(query)

# Enter query here
query = """
select
      n_name,
      sum(l_extendedprice * (1 - l_discount)) as revenue
    from
      customer,
      orders,
      lineitem,
      supplier,
      nation,
      region
    where
      c_custkey = o_custkey
      and l_orderkey = o_orderkey
      and l_suppkey = s_suppkey
      and c_nationkey = s_nationkey
      and s_nationkey = n_nationkey
      and n_regionkey = r_regionkey
      and r_name = 'ASIA'
      and o_orderdate >= '1994-01-01'
      and o_orderdate < '1995-01-01'
      and c_acctbal > 10
      and s_acctbal > 20
    group by
      n_name
    order by
      revenue desc;
"""

# query = """
# select * from customer
#  join nation on customer.c_nationkey = nation.n_nationkey
#  order by nation.n_nationkey;
# """


# Execute the query
# cur.execute(query)

# Execute EXPLAIN statement to get QEP
explain_query = "EXPLAIN (FORMAT JSON) " + query
cur.execute(explain_query)

# Fetch and print the QEP
qep_result = cur.fetchone()
print("Query Execution Plan (QEP):")
print(qep_result[0])

# Build the query plan tree
root_node = build_query_plan_tree(qep_result[0][0])

# Print the query plan tree
# print_query_plan_tree(root_node)

query_plan_string = get_query_plan_tree(root_node)
print(query_plan_string)

# Store query plan results in a json file
with open('queryplan.json', 'w') as f:
    json.dump(qep_result[0], f)

# Calculate the costs and generate the explanations for each node in the query plan tree
calculate_cost(conn, root_node)

cost_string = get_costs_info(root_node)
print(cost_string)

explanation_string = get_cost_explanation(root_node)
print(explanation_string)


# TODO: Merging with frontend
# TODO: Function that takes in the 5 arguments for login and returns true or false depending on login status
# Host, Port, Database, Username, Password
def connect_to_database(host, port, database, username, password):
    try:
        # Establish a connection to the PostgreSQL database
        conn = psycopg2.connect(
            host=host,
            port=port,
            database=database,
            user=username,
            password=password
        )
        # If connection is successful, return True and the connection object
        return True, conn
    except psycopg2.Error as e:
        # If an error occurs during connection, print the error and return False
        print("Error:", e)
        return False, None


# TODO: Function that takes in query string, and return one array of three strings
# 1. QEP Tree
# 2. Actual vs Calculated Cost
# 3. Cost Explanation
def explain_query(connection, query):
    output_array = []

    # Using the connection and query, get the QEP from the database
    cur = connection.cursor()

    # Turn off parallel query execution
    # Disable parallel query execution
    query = """
    SET max_parallel_workers_per_gather = 0;
    """

    cur.execute(query)

    # Execute EXPLAIN statement to get QEP
    explain_query = "EXPLAIN (FORMAT JSON) " + query

    # Check if query is a proper query
    try:
        cur.execute(explain_query)
        # Fetch and print the QEP
        qep_result = cur.fetchone()
    except psycopg2.Error as e:
        # Handle the error
        print("Error:", e)
        return "Failed Query. Reset and try again."

    # Build the query plan tree
    root_node = build_query_plan_tree(qep_result[0][0])

    # Return the query plan tree
    query_plan_tree_string = get_query_plan_tree(root_node)
    output_array[0] = query_plan_tree_string

    # 2. Format Actual vs Calculated Cost string
    calculate_cost(connection, root_node)

    cost_string = get_costs_info(root_node)
    output_array[1] = cost_string

    # 3. Format Cost Explanation string
    explanation_string = get_cost_explanation(root_node)
    output_array[2] = explanation_string

    return output_array


# Fetch the results
# results = cur.fetchall()

# Print the results
# for row in results:
#     print(row)

# Close the cursor and connection
cur.close()
conn.close()